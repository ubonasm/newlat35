import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from janome.tokenizer import Tokenizer
from collections import Counter, defaultdict
import networkx as nx
import re
from io import StringIO
import json

# Groq API (ç„¡æ–™æ ã‚ã‚Š)
import requests

st.set_page_config(page_title="æˆæ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ", layout="wide", page_icon="ğŸ“š")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'analyzed_data' not in st.session_state:
    st.session_state.analyzed_data = None
if 'segments' not in st.session_state:
    st.session_state.segments = None

class ClassroomAnalyzer:
    def __init__(self, custom_dict=None):
        self.tokenizer = Tokenizer()
        self.custom_dict = custom_dict or {}
        
    def load_custom_dictionary(self, dict_df):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’èª­ã¿è¾¼ã‚€"""
        custom_dict = {}
        for _, row in dict_df.iterrows():
            word = str(row.iloc[0]).strip()
            reading = str(row.iloc[1]).strip() if len(row) > 1 else word
            custom_dict[word] = reading
        return custom_dict
    
    def tokenize_with_custom_dict(self, text):
        """ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’å„ªå…ˆã—ã¦å½¢æ…‹ç´ è§£æ"""
        tokens = []
        remaining_text = text
        
        # ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã®èªã‚’å„ªå…ˆçš„ã«æ¤œå‡º
        for word in sorted(self.custom_dict.keys(), key=len, reverse=True):
            if word in remaining_text:
                parts = remaining_text.split(word)
                new_remaining = []
                for i, part in enumerate(parts):
                    if i > 0:
                        tokens.append({
                            'surface': word,
                            'reading': self.custom_dict[word],
                            'pos': 'ã‚«ã‚¹ã‚¿ãƒ '
                        })
                    new_remaining.append(part)
                remaining_text = '###CUSTOM###'.join(new_remaining)
        
        # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é€šå¸¸ã®å½¢æ…‹ç´ è§£æ
        text_parts = remaining_text.split('###CUSTOM###')
        final_tokens = []
        token_idx = 0
        
        for part in text_parts:
            if part:
                for token in self.tokenizer.tokenize(part):
                    parts = str(token).split('\t')
                    surface = parts[0]
                    features = parts[1].split(',') if len(parts) > 1 else []
                    
                    final_tokens.append({
                        'surface': surface,
                        'reading': features[7] if len(features) > 7 else surface,
                        'pos': features[0] if features else 'æœªçŸ¥èª'
                    })
            
            if token_idx < len(tokens):
                final_tokens.append(tokens[token_idx])
                token_idx += 1
        
        return final_tokens
    
    def analyze_speakers(self, df):
        """è©±è€…ã”ã¨ã®ç™ºè¨€ã‚’åˆ†æ"""
        speaker_analysis = defaultdict(lambda: {'utterances': [], 'word_freq': Counter()})
        
        for _, row in df.iterrows():
            speaker = str(row['Speaker']).strip()
            utterance = str(row['Utterance']).strip()
            
            tokens = self.tokenize_with_custom_dict(utterance)
            words = [t['surface'] for t in tokens if t['pos'] in ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'ã‚«ã‚¹ã‚¿ãƒ ']]
            
            speaker_analysis[speaker]['utterances'].append(utterance)
            speaker_analysis[speaker]['word_freq'].update(words)
        
        return speaker_analysis
    
    def segment_classroom(self, df, groq_api_key):
        """AIã‚’ä½¿ã£ã¦æˆæ¥­ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²"""
        # æˆæ¥­è¨˜éŒ²ã‚’çµåˆ
        full_text = ""
        for idx, row in df.iterrows():
            full_text += f"[{row['No']}] {row['Speaker']}: {row['Utterance']}\n"
        
        # Groq APIã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
        prompt = f"""ä»¥ä¸‹ã®æˆæ¥­è¨˜éŒ²ã‚’åˆ†æã—ã€ãƒ†ãƒ¼ãƒã‚„å†…å®¹ã®å¤‰åŒ–ã«åŸºã¥ã„ã¦3ã€œ7å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆæ„å‘³ã®ã‚ã‚‹ã¾ã¨ã¾ã‚Šï¼‰ã«åˆ†ã‘ã¦ãã ã•ã„ã€‚
å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã¯ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- segment_id: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç•ªå·ï¼ˆ1ã‹ã‚‰é–‹å§‹ï¼‰
- start_no: é–‹å§‹ç™ºè¨€ç•ªå·
- end_no: çµ‚äº†ç™ºè¨€ç•ªå·
- theme: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ†ãƒ¼ãƒï¼ˆ20æ–‡å­—ä»¥å†…ï¼‰
- summary: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è¦ç´„ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

æˆæ¥­è¨˜éŒ²:
{full_text[:3000]}

å‡ºåŠ›å½¢å¼:
{{"segments": [{{"segment_id": 1, "start_no": 1, "end_no": 5, "theme": "å°å…¥", "summary": "æˆæ¥­ã®ç›®æ¨™ã‚’èª¬æ˜"}}]}}
"""
        
        try:
            response = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {groq_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "llama-3.3-70b-versatile",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.3,
                    "max_tokens": 2000
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # JSONã‚’æŠ½å‡º
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    segments_data = json.loads(json_match.group())
                    return segments_data.get('segments', [])
            
        except Exception as e:
            st.warning(f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å‡ç­‰åˆ†å‰²
        total_rows = len(df)
        segment_size = max(total_rows // 5, 1)
        segments = []
        for i in range(0, total_rows, segment_size):
            segments.append({
                'segment_id': len(segments) + 1,
                'start_no': int(df.iloc[i]['No']),
                'end_no': int(df.iloc[min(i + segment_size - 1, total_rows - 1)]['No']),
                'theme': f'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {len(segments) + 1}',
                'summary': 'è‡ªå‹•åˆ†å‰²'
            })
        
        return segments
    
    def analyze_segments(self, df, segments):
        """å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ä¸»è¦èªã‚’åˆ†æ"""
        segment_analysis = []
        
        for seg in segments:
            seg_df = df[(df['No'] >= seg['start_no']) & (df['No'] <= seg['end_no'])]
            
            all_words = []
            for _, row in seg_df.iterrows():
                tokens = self.tokenize_with_custom_dict(str(row['Utterance']))
                words = [t['surface'] for t in tokens if t['pos'] in ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'ã‚«ã‚¹ã‚¿ãƒ '] and len(t['surface']) > 1]
                all_words.extend(words)
            
            word_freq = Counter(all_words)
            top_words = word_freq.most_common(20)
            
            segment_analysis.append({
                'segment_id': seg['segment_id'],
                'theme': seg['theme'],
                'summary': seg['summary'],
                'top_words': top_words,
                'total_words': len(all_words),
                'unique_words': len(set(all_words))
            })
        
        return segment_analysis
    
    def analyze_word_transitions(self, df, segments, segment_analysis):
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®èªã®é·ç§»ã‚’åˆ†æ"""
        transitions = []
        
        for i in range(len(segment_analysis) - 1):
            current_seg = segment_analysis[i]
            next_seg = segment_analysis[i + 1]
            
            current_words = set([w[0] for w in current_seg['top_words'][:10]])
            next_words = set([w[0] for w in next_seg['top_words'][:10]])
            
            # å…±é€šèªï¼ˆå¼•ãç¶™ãŒã‚ŒãŸèªï¼‰
            common_words = current_words & next_words
            
            # å½±éŸ¿åŠ›ã‚¹ã‚³ã‚¢
            influence_score = len(common_words) / len(current_words) if current_words else 0
            
            transitions.append({
                'from_segment': current_seg['segment_id'],
                'to_segment': next_seg['segment_id'],
                'common_words': list(common_words),
                'influence_score': influence_score
            })
        
        return transitions

# Streamlit UI
st.title("ğŸ“š æˆæ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("å½¢æ…‹ç´ è§£æã¨AIã‚’æ´»ç”¨ã—ãŸæˆæ¥­è¨˜éŒ²ã®åˆ†æãƒ„ãƒ¼ãƒ«")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # Groq APIã‚­ãƒ¼å…¥åŠ›
    groq_api_key = st.text_input(
        "Groq APIã‚­ãƒ¼ï¼ˆç„¡æ–™ï¼‰",
        type="password",
        help="https://console.groq.com ã§APIã‚­ãƒ¼ã‚’å–å¾—ã§ãã¾ã™ï¼ˆç„¡æ–™æ ã‚ã‚Šï¼‰"
    )
    
    st.markdown("---")
    st.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    1. Groq APIã‚­ãƒ¼ã‚’å…¥åŠ›
    2. æˆæ¥­è¨˜éŒ²CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    3. ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    4. åˆ†æã‚’å®Ÿè¡Œ
    """)

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿", "ğŸ‘¥ è©±è€…åˆ†æ", "ğŸ“Š ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ", "ğŸ”„ èªã®é·ç§»"])

with tab1:
    st.header("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("æˆæ¥­è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«")
        classroom_file = st.file_uploader(
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆNo, Speaker, Utteranceï¼‰",
            type=['csv'],
            key='classroom'
        )
        
        if classroom_file:
            try:
                df = pd.read_csv(classroom_file)
                st.success(f"âœ… {len(df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.dataframe(df.head(10), use_container_width=True)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                df = None
    
    with col2:
        st.subheader("ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        dict_file = st.file_uploader(
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆèª, èª­ã¿æ–¹ï¼‰",
            type=['csv'],
            key='dictionary'
        )
        
        custom_dict_df = None
        if dict_file:
            try:
                custom_dict_df = pd.read_csv(dict_file, header=None)
                st.success(f"âœ… {len(custom_dict_df)}èªã®è¾æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.dataframe(custom_dict_df.head(10), use_container_width=True)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    
    st.markdown("---")
    
    if st.button("ğŸš€ åˆ†æã‚’é–‹å§‹", type="primary", use_container_width=True):
        if not groq_api_key:
            st.error("Groq APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        elif classroom_file is None:
            st.error("æˆæ¥­è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            with st.spinner("åˆ†æä¸­..."):
                # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
                analyzer = ClassroomAnalyzer()
                
                # ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸èª­ã¿è¾¼ã¿
                if custom_dict_df is not None:
                    analyzer.custom_dict = analyzer.load_custom_dictionary(custom_dict_df)
                    st.info(f"ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸: {len(analyzer.custom_dict)}èªã‚’é©ç”¨")
                
                # è©±è€…åˆ†æ
                speaker_analysis = analyzer.analyze_speakers(df)
                
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²
                segments = analyzer.segment_classroom(df, groq_api_key)
                
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
                segment_analysis = analyzer.analyze_segments(df, segments)
                
                # èªã®é·ç§»åˆ†æ
                transitions = analyzer.analyze_word_transitions(df, segments, segment_analysis)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state.analyzed_data = {
                    'df': df,
                    'speaker_analysis': speaker_analysis,
                    'segments': segments,
                    'segment_analysis': segment_analysis,
                    'transitions': transitions,
                    'analyzer': analyzer
                }
                
                st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼ä»–ã®ã‚¿ãƒ–ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

with tab2:
    st.header("ğŸ‘¥ è©±è€…åˆ†æ")
    
    if st.session_state.analyzed_data:
        data = st.session_state.analyzed_data
        speaker_analysis = data['speaker_analysis']
        
        st.subheader("è©±è€…ã”ã¨ã®ä¸»å¼µã¨ç‰¹å¾´")
        
        for speaker, info in speaker_analysis.items():
            with st.expander(f"ğŸ—£ï¸ {speaker} ï¼ˆç™ºè¨€æ•°: {len(info['utterances'])}ï¼‰"):
                st.markdown("**ä¸»è¦ãªèª:**")
                top_words = info['word_freq'].most_common(15)
                
                # èªã®é »åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                if top_words:
                    words_df = pd.DataFrame(top_words, columns=['èª', 'é »åº¦'])
                    fig = px.bar(words_df, x='èª', y='é »åº¦', title=f'{speaker}ã®ä¸»è¦èª')
                    st.plotly_chart(fig, use_container_width=True, key=f"speaker_chart_{speaker}")
                
                st.markdown("**ç™ºè¨€ä¾‹:**")
                for i, utterance in enumerate(info['utterances'][:3], 1):
                    st.markdown(f"{i}. {utterance}")
    else:
        st.info("ã¾ãšã€Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€ã‚¿ãƒ–ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

with tab3:
    st.header("ğŸ“Š ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ")
    
    if st.session_state.analyzed_data:
        data = st.session_state.analyzed_data
        segments = data['segments']
        segment_analysis = data['segment_analysis']
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–¢ä¿‚å›³
        st.subheader("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æµã‚Œ")
        
        G = nx.DiGraph()
        for seg in segment_analysis:
            G.add_node(seg['segment_id'], label=seg['theme'])
        
        for i in range(len(segment_analysis) - 1):
            G.add_edge(segment_analysis[i]['segment_id'], segment_analysis[i+1]['segment_id'])
        
        pos = nx.spring_layout(G)
        
        edge_x = []
        edge_y = []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=2, color='#888'),
            hoverinfo='none',
            mode='lines')
        
        node_x = []
        node_y = []
        node_text = []
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(G.nodes[node]['label'])
        
        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="top center",
            hoverinfo='text',
            marker=dict(
                size=30,
                color='lightblue',
                line=dict(width=2, color='darkblue')))
        
        fig = go.Figure(data=[edge_trace, node_trace],
                       layout=go.Layout(
                           showlegend=False,
                           hovermode='closest',
                           margin=dict(b=0,l=0,r=0,t=0),
                           xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           height=400))
        
        st.plotly_chart(fig, use_container_width=True, key="segment_flow_graph")
        
        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©³ç´°
        st.subheader("å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©³ç´°")
        
        for seg in segment_analysis:
            with st.expander(f"ğŸ“Œ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {seg['segment_id']}: {seg['theme']}"):
                st.markdown(f"**è¦ç´„:** {seg['summary']}")
                st.markdown(f"**ç·èªæ•°:** {seg['total_words']} | **ãƒ¦ãƒ‹ãƒ¼ã‚¯èªæ•°:** {seg['unique_words']}")
                
                st.markdown("**ä¸»è¦èªï¼ˆä¸Šä½20èªï¼‰:**")
                words_df = pd.DataFrame(seg['top_words'], columns=['èª', 'é »åº¦'])
                
                col1, col2 = st.columns([2, 1])
                with col1:
                    fig = px.bar(words_df.head(10), x='èª', y='é »åº¦', 
                                title=f'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{seg["segment_id"]}ã®ä¸»è¦èª')
                    st.plotly_chart(fig, use_container_width=True, key=f"segment_words_{seg['segment_id']}")
                
                with col2:
                    st.dataframe(words_df, use_container_width=True, height=400)
    else:
        st.info("ã¾ãšã€Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€ã‚¿ãƒ–ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

with tab4:
    st.header("ğŸ”„ èªã®é·ç§»åˆ†æ")
    
    if st.session_state.analyzed_data:
        data = st.session_state.analyzed_data
        transitions = data['transitions']
        segment_analysis = data['segment_analysis']
        
        st.subheader("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®èªã®å¼•ãç¶™ã")
        
        # é·ç§»ã®å¯è¦–åŒ–
        for trans in transitions:
            from_seg = segment_analysis[trans['from_segment'] - 1]
            to_seg = segment_analysis[trans['to_segment'] - 1]
            
            influence_pct = trans['influence_score'] * 100
            
            with st.expander(f"ğŸ”€ {from_seg['theme']} â†’ {to_seg['theme']} ï¼ˆå½±éŸ¿åŠ›: {influence_pct:.1f}%ï¼‰"):
                st.markdown(f"**å¼•ãç¶™ãŒã‚ŒãŸèª:** {len(trans['common_words'])}èª")
                
                if trans['common_words']:
                    st.markdown("**å…±é€šèª:**")
                    st.write(", ".join(trans['common_words']))
                    
                    # å½±éŸ¿åŠ›ãƒ¡ãƒ¼ã‚¿ãƒ¼
                    fig = go.Figure(go.Indicator(
                        mode="gauge+number",
                        value=influence_pct,
                        title={'text': "å½±éŸ¿åŠ›ã‚¹ã‚³ã‚¢"},
                        gauge={'axis': {'range': [None, 100]},
                               'bar': {'color': "darkblue"},
                               'steps': [
                                   {'range': [0, 30], 'color': "lightgray"},
                                   {'range': [30, 70], 'color': "gray"},
                                   {'range': [70, 100], 'color': "lightblue"}],
                               'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 90}}))
                    
                    fig.update_layout(height=250)
                    st.plotly_chart(fig, use_container_width=True, key=f"influence_gauge_{trans['from_segment']}_{trans['to_segment']}")
                else:
                    st.info("å…±é€šèªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ†ãƒ¼ãƒãŒå¤§ããå¤‰åŒ–ã—ã¦ã„ã¾ã™ã€‚")
        
        # å…¨ä½“ã®é·ç§»ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        st.subheader("é·ç§»ãƒãƒˆãƒªãƒƒã‚¯ã‚¹")
        
        matrix_data = []
        for trans in transitions:
            matrix_data.append({
                'From': f"S{trans['from_segment']}",
                'To': f"S{trans['to_segment']}",
                'Score': trans['influence_score']
            })
        
        if matrix_data:
            matrix_df = pd.DataFrame(matrix_data)
            fig = px.bar(matrix_df, x='From', y='Score', color='To',
                        title='ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®å½±éŸ¿åŠ›ã‚¹ã‚³ã‚¢',
                        labels={'Score': 'å½±éŸ¿åŠ›', 'From': 'å…ƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆ'})
            st.plotly_chart(fig, use_container_width=True, key="transition_matrix")
    else:
        st.info("ã¾ãšã€Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€ã‚¿ãƒ–ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("ğŸ’¡ **ãƒ’ãƒ³ãƒˆ:** Groq APIã¯ç„¡æ–™æ ãŒã‚ã‚Šã¾ã™ã€‚[console.groq.com](https://console.groq.com)ã§ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚")
